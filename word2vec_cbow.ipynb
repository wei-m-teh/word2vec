{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/weiteh/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/weiteh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download('punkt')\n",
    "bible = gutenberg.sents('bible-kjv.txt') \n",
    "remove_terms = punctuation + '0123456789'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all words and filter the punctuation and numbers\n",
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [ \" \".join(x) for x in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/weiteh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = filter(None, normalize_corpus(norm_bible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 30103\n",
      "\n",
      "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "\n",
      "Processed line: god said let firmament midst waters let divide waters waters\n"
     ]
    }
   ],
   "source": [
    "print('Total lines:', len(bible))\n",
    "print('\\nSample line:', bible[10])\n",
    "print('\\nProcessed line:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/word2vec-cbow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/weiteh/word2vec/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/weiteh/word2vec/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/weiteh/word2vec/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/weiteh/word2vec/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/weiteh/word2vec/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/weiteh/word2vec/venv/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(norm_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12425\n",
      "Vocabulary Sample: [('sinite', 6780), ('munitions', 10827), ('tempt', 2691), ('zelophehad', 3143), ('zadok', 1037), ('ascribe', 5999), ('window', 2453), ('worketh', 1428), ('rolled', 2953), ('anakims', 3516)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            # vocab_size + 1 to avoid exception when to_categorical returns the last index (12424)\n",
    "            # 0 is a reserved index\n",
    "            y = np_utils.to_categorical(label_word, vocab_size) \n",
    "            \n",
    "            yield (x, y)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
      "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
      "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
      "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
      "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
      "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
      "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
      "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
      "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
      "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
      "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
     ]
    }
   ],
   "source": [
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            1242500   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# view model summary\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"294pt\" viewBox=\"0.00 0.00 293.00 294.00\" width=\"293pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 290)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-290 289,-290 289,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139944019614688 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139944019614688</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 285,-212.5 285,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"49\" y=\"-185.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"98,-166.5 98,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"98,-189.5 166,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"166,-166.5 166,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-197.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"166,-189.5 285,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-174.3\">(None, 4, 100)</text>\n",
       "</g>\n",
       "<!-- 139944019615360 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139944019615360</title>\n",
       "<polygon fill=\"none\" points=\"12,-83.5 12,-129.5 273,-129.5 273,-83.5 12,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"49\" y=\"-102.8\">Lambda</text>\n",
       "<polyline fill=\"none\" points=\"86,-83.5 86,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"86,-106.5 154,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"154,-83.5 154,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213.5\" y=\"-114.3\">(None, 4, 100)</text>\n",
       "<polyline fill=\"none\" points=\"154,-106.5 273,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213.5\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139944019614688&#45;&gt;139944019615360 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139944019614688-&gt;139944019615360</title>\n",
       "<path d=\"M142.5,-166.366C142.5,-158.152 142.5,-148.658 142.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"146,-139.607 142.5,-129.607 139,-139.607 146,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139944019640448 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139944019640448</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-0.5 18.5,-46.5 266.5,-46.5 266.5,-0.5 18.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"49\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"79.5,-0.5 79.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"79.5,-23.5 147.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"147.5,-0.5 147.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"147.5,-23.5 266.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207\" y=\"-8.3\">(None, 12425)</text>\n",
       "</g>\n",
       "<!-- 139944019615360&#45;&gt;139944019640448 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139944019615360-&gt;139944019640448</title>\n",
       "<path d=\"M142.5,-83.3664C142.5,-75.1516 142.5,-65.6579 142.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"146,-56.6068 142.5,-46.6068 139,-56.6069 146,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139944019615472 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139944019615472</title>\n",
       "<polygon fill=\"none\" points=\"66.5,-249.5 66.5,-285.5 218.5,-285.5 218.5,-249.5 66.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"142.5\" y=\"-263.8\">139944019615472</text>\n",
       "</g>\n",
       "<!-- 139944019615472&#45;&gt;139944019614688 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139944019615472-&gt;139944019614688</title>\n",
       "<path d=\"M142.5,-249.254C142.5,-241.363 142.5,-231.749 142.5,-222.602\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"146,-222.591 142.5,-212.591 139,-222.591 146,-222.591\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 1 \tLoss: 848516.8165124105\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 1 \tLoss: 1904098.162887712\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 1 \tLoss: 3082571.9013154404\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 2 \tLoss: 1047760.6006003421\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 2 \tLoss: 2201740.7181942514\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 2 \tLoss: 3351353.3083837815\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 3 \tLoss: 1054670.6652652072\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 3 \tLoss: 2206707.5963705657\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 3 \tLoss: 3347636.6375550968\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 4 \tLoss: 1043625.9000906495\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 4 \tLoss: 2185883.75619327\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 4 \tLoss: 3330653.64733051\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 5 \tLoss: 1065997.9725992712\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 5 \tLoss: 2226847.152746957\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 5 \tLoss: 3358707.20405399\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 6 \tLoss: 1055878.8962014348\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 6 \tLoss: 2204746.2153424523\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 6 \tLoss: 3356474.867673686\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 7 \tLoss: 1090124.188415133\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 7 \tLoss: 2272541.5161324623\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 7 \tLoss: 3450133.266914378\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 8 \tLoss: 1101621.2806870048\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 8 \tLoss: 2289233.664581396\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 8 \tLoss: 3474771.176103643\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 9 \tLoss: 1122413.268274741\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 9 \tLoss: 2328111.642570034\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 9 \tLoss: 3535950.3984776274\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 10 \tLoss: 1135946.910314185\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 10 \tLoss: 2355985.545037288\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 10 \tLoss: 3571574.8719949652\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 11 \tLoss: 1141227.0785186482\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 11 \tLoss: 2368302.1341064572\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 11 \tLoss: 3609650.481118377\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 12 \tLoss: 1134198.4904716248\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 12 \tLoss: 2353569.589389505\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 12 \tLoss: 3578620.2688382994\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 13 \tLoss: 1150311.0495048345\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 13 \tLoss: 2381051.0958419787\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 13 \tLoss: 3613684.9798694416\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 14 \tLoss: 1180304.147298086\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 14 \tLoss: 2428668.4383510267\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 14 \tLoss: 3683180.5355247385\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 15 \tLoss: 1177299.3524561825\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 15 \tLoss: 2424510.367055608\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 15 \tLoss: 3675083.5734996223\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 16 \tLoss: 1202914.6897195335\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 16 \tLoss: 2467553.308677485\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 16 \tLoss: 3733143.7453527558\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 17 \tLoss: 1213842.642884766\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 17 \tLoss: 2474400.749100884\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 17 \tLoss: 3731508.441872705\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 18 \tLoss: 1221420.391240437\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 18 \tLoss: 2494527.9758870127\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 18 \tLoss: 3761270.7046377384\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Epoch: 19 \tLoss: 1213600.2020222824\n",
      "Processed 200000 (context, word) pairs\n",
      "Epoch: 19 \tLoss: 2485576.7925917585\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 19 \tLoss: 3751997.6315194154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "            print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>-0.252762</td>\n",
       "      <td>0.875928</td>\n",
       "      <td>1.333759</td>\n",
       "      <td>1.317303</td>\n",
       "      <td>0.726024</td>\n",
       "      <td>-4.161635</td>\n",
       "      <td>0.042620</td>\n",
       "      <td>0.072045</td>\n",
       "      <td>0.036135</td>\n",
       "      <td>1.271854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049555</td>\n",
       "      <td>-0.698928</td>\n",
       "      <td>2.006484</td>\n",
       "      <td>1.787941</td>\n",
       "      <td>-0.507635</td>\n",
       "      <td>-0.771421</td>\n",
       "      <td>-0.857663</td>\n",
       "      <td>0.847739</td>\n",
       "      <td>-0.239112</td>\n",
       "      <td>0.466033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>-1.000185</td>\n",
       "      <td>-0.443811</td>\n",
       "      <td>-0.853266</td>\n",
       "      <td>2.256062</td>\n",
       "      <td>-2.822240</td>\n",
       "      <td>-1.663752</td>\n",
       "      <td>3.925170</td>\n",
       "      <td>1.050252</td>\n",
       "      <td>1.687156</td>\n",
       "      <td>2.633649</td>\n",
       "      <td>...</td>\n",
       "      <td>1.775536</td>\n",
       "      <td>-1.998470</td>\n",
       "      <td>-0.504991</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.231358</td>\n",
       "      <td>-2.256495</td>\n",
       "      <td>-1.003457</td>\n",
       "      <td>-0.209728</td>\n",
       "      <td>0.340843</td>\n",
       "      <td>-1.187086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>1.190516</td>\n",
       "      <td>-1.641793</td>\n",
       "      <td>-0.670725</td>\n",
       "      <td>1.248516</td>\n",
       "      <td>-1.818018</td>\n",
       "      <td>-2.642782</td>\n",
       "      <td>0.516240</td>\n",
       "      <td>-0.181705</td>\n",
       "      <td>1.149439</td>\n",
       "      <td>0.897360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229912</td>\n",
       "      <td>-2.757271</td>\n",
       "      <td>0.919006</td>\n",
       "      <td>-0.429050</td>\n",
       "      <td>0.111108</td>\n",
       "      <td>-0.480288</td>\n",
       "      <td>-0.485003</td>\n",
       "      <td>-0.069682</td>\n",
       "      <td>0.696978</td>\n",
       "      <td>0.167227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>-0.898649</td>\n",
       "      <td>0.477333</td>\n",
       "      <td>-1.633646</td>\n",
       "      <td>0.902412</td>\n",
       "      <td>-0.245227</td>\n",
       "      <td>-0.253196</td>\n",
       "      <td>-0.005445</td>\n",
       "      <td>-0.500010</td>\n",
       "      <td>-0.500856</td>\n",
       "      <td>1.635019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.526804</td>\n",
       "      <td>0.332105</td>\n",
       "      <td>-0.078667</td>\n",
       "      <td>-0.549390</td>\n",
       "      <td>0.153660</td>\n",
       "      <td>-0.396543</td>\n",
       "      <td>1.092445</td>\n",
       "      <td>-1.214475</td>\n",
       "      <td>0.153579</td>\n",
       "      <td>0.774406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>-0.397165</td>\n",
       "      <td>-0.121370</td>\n",
       "      <td>0.675113</td>\n",
       "      <td>-0.558662</td>\n",
       "      <td>1.576094</td>\n",
       "      <td>0.716682</td>\n",
       "      <td>-1.289818</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-1.082852</td>\n",
       "      <td>-0.440885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045840</td>\n",
       "      <td>1.268043</td>\n",
       "      <td>-0.298018</td>\n",
       "      <td>1.130146</td>\n",
       "      <td>-0.661818</td>\n",
       "      <td>-1.250847</td>\n",
       "      <td>1.355970</td>\n",
       "      <td>0.376677</td>\n",
       "      <td>-0.871604</td>\n",
       "      <td>-0.259294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "shall -0.252762  0.875928  1.333759  1.317303  0.726024 -4.161635  0.042620   \n",
       "unto  -1.000185 -0.443811 -0.853266  2.256062 -2.822240 -1.663752  3.925170   \n",
       "lord   1.190516 -1.641793 -0.670725  1.248516 -1.818018 -2.642782  0.516240   \n",
       "thou  -0.898649  0.477333 -1.633646  0.902412 -0.245227 -0.253196 -0.005445   \n",
       "thy   -0.397165 -0.121370  0.675113 -0.558662  1.576094  0.716682 -1.289818   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "shall  0.072045  0.036135  1.271854  ... -0.049555 -0.698928  2.006484   \n",
       "unto   1.050252  1.687156  2.633649  ...  1.775536 -1.998470 -0.504991   \n",
       "lord  -0.181705  1.149439  0.897360  ... -0.229912 -2.757271  0.919006   \n",
       "thou  -0.500010 -0.500856  1.635019  ... -0.526804  0.332105 -0.078667   \n",
       "thy   -0.004304 -1.082852 -0.440885  ... -0.045840  1.268043 -0.298018   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "shall  1.787941 -0.507635 -0.771421 -0.857663  0.847739 -0.239112  0.466033  \n",
       "unto   0.363600  0.231358 -2.256495 -1.003457 -0.209728  0.340843 -1.187086  \n",
       "lord  -0.429050  0.111108 -0.480288 -0.485003 -0.069682  0.696978  0.167227  \n",
       "thou  -0.549390  0.153660 -0.396543  1.092445 -1.214475  0.153579  0.774406  \n",
       "thy    1.130146 -0.661818 -1.250847  1.355970  0.376677 -0.871604 -0.259294  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 12424)\n"
     ]
    }
   ],
   "source": [
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 10, 46,  6, 39])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix[word2id['shall'] - 1].argsort()[1:6]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:10]+1] \n",
    "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'egypt': ['thither',\n",
       "  'led',\n",
       "  'lot',\n",
       "  'valley',\n",
       "  'meet',\n",
       "  'camp',\n",
       "  'whither',\n",
       "  'gilead',\n",
       "  'north'],\n",
       " 'famine': ['inherit',\n",
       "  'yield',\n",
       "  'continue',\n",
       "  'strangers',\n",
       "  'signs',\n",
       "  'flood',\n",
       "  'pestilence',\n",
       "  'cease',\n",
       "  'lived'],\n",
       " 'god': ['good',\n",
       "  'may',\n",
       "  'glory',\n",
       "  'let',\n",
       "  'would',\n",
       "  'cause',\n",
       "  'time',\n",
       "  'aaron',\n",
       "  'judgment'],\n",
       " 'gospel': ['remaineth',\n",
       "  'vision',\n",
       "  'tongues',\n",
       "  'preached',\n",
       "  'hearing',\n",
       "  'scriptures',\n",
       "  'preach',\n",
       "  'queen',\n",
       "  'resurrection'],\n",
       " 'jesus': ['law',\n",
       "  'dead',\n",
       "  'world',\n",
       "  'stood',\n",
       "  'chief',\n",
       "  'jews',\n",
       "  'mother',\n",
       "  'temple',\n",
       "  'gate'],\n",
       " 'john': ['wept',\n",
       "  'howbeit',\n",
       "  'remained',\n",
       "  'laban',\n",
       "  'sanctified',\n",
       "  'officers',\n",
       "  'peter',\n",
       "  'dream',\n",
       "  'abimelech'],\n",
       " 'moses': ['pharaoh',\n",
       "  'servants',\n",
       "  'lo',\n",
       "  'became',\n",
       "  'wilderness',\n",
       "  'angel',\n",
       "  'received',\n",
       "  'husband',\n",
       "  'gate'],\n",
       " 'noah': ['ninety',\n",
       "  'shem',\n",
       "  'barnabas',\n",
       "  'fruitful',\n",
       "  'plenteous',\n",
       "  'joppa',\n",
       "  'freely',\n",
       "  'sixty',\n",
       "  'soldiers']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
